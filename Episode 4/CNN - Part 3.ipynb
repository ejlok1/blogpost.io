{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "# A picture spoken with 1000 words, reported by CNN - Part 3 \n",
    "Eu Jin Lok\n",
    "\n",
    "9 February 2018\n",
    "\n",
    "# CNN training \n",
    "In this notebook we will go into the details of how to build a document classifier using CNN, a deep learning architecture well known for images classification. For the full background on this topic, please checkout my blog post in this link: \n",
    "\n",
    "xxxxxxxxxxx\n",
    "\n",
    "This is part 3 of the code which looks at building the CNN model, with the embedding layer using our pretrained GloVe vectors from part 2 of the code. More information of how CNN can be applied to text data: \n",
    "\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    " \n",
    "So without further ado, lets begin...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the key libraries \n",
    "import pandas as pd \n",
    "from pandas import crosstab\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, MaxPooling1D, Convolution1D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\CNN\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first step after loadings the necessary packages, we'll go grab our training dataset again and run through the same data processing steps again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23313 unique tokens or words.\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "train = pd.read_csv(\"happydb\\\\cleaned_hm.csv\")  \n",
    "\n",
    "# Lets one-hot encode the labels  \n",
    "labels=train.predicted_category.unique()\n",
    "dic={}\n",
    "for i,labels in enumerate(labels):\n",
    "    dic[labels]=i\n",
    "labels=train.predicted_category.apply(lambda x:dic[x])\n",
    "\n",
    "val=train.sample(frac=0.2,random_state=200)\n",
    "train=train.drop(val.index)\n",
    "\n",
    "NUM_WORDS=20000 # if set, tokenization will be restricted to the top num_words most common words in the dataset).\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "\n",
    "# we need to fit the tokenizer on our text data in order to get the tokens\n",
    "texts=train.cleaned_hm\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Found %s unique tokens or words.' % len(word_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the words the sentences in our documents to the index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a successful date with someone I felt sympathy and connection with.\n",
      "[1, 23, 16, 3, 758, 315, 13, 284, 1, 94, 9298, 5, 2393, 13]\n",
      "315 = index for the word 'Date' \n",
      "Date is an index number of 315. And it appears in the right position (5th) in the sentence\n"
     ]
    }
   ],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(texts) # converts the text to numbers essentially\n",
    "sequences_valid=tokenizer.texts_to_sequences(val.cleaned_hm)\n",
    "word_index = tokenizer.word_index\n",
    "#Although word_index contains all words tokenizer.texts_to_sequences takes num_words into account.\n",
    "\n",
    "# Check the index is working correctly \n",
    "print(texts[0])\n",
    "print(sequences_train[0])\n",
    "print(word_index['date'],\"= index for the word 'Date' \") \n",
    "print('Date is an index number of 315. And it appears in the right position (5th) in the sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting the data into CNN, and we need to ensure the shape of the dataset is the same across all text. But because each text varies in lenght, we'll cap it at a fixed lenght, and just pad it with zeros to fill in the gaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    1   23   16    3  758  315   13  284    1\n",
      "   94 9298    5 2393   13]\n"
     ]
    }
   ],
   "source": [
    "# set the sequence length of the text to speed up training and prevent overfitting. \n",
    "seq_len = 500\n",
    "X_train = pad_sequences(sequences_train,maxlen=seq_len, value=0)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=seq_len, value=0)\n",
    "\n",
    "# Lets check a single record to see how it looks\n",
    "print(X_train[0]) # By default we pad the left side. In order words, all the text is right side aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last part of the processing is to one-hot encode / binarise the target. That's the format that works well with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80428, 7)\n",
      "(20107, 7)\n"
     ]
    }
   ],
   "source": [
    "y_train =train.predicted_category.apply(lambda x:dic[x])\n",
    "y_train = to_categorical(np.asarray(labels[train.index]))\n",
    "y_val =val.predicted_category.apply(lambda x:dic[x])\n",
    "y_val = to_categorical(np.asarray(labels[y_val.index]))\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - No pretrained word embedding = 95% accuracy\n",
    "And here we are. Lets start with a CNN without using a pretrained embedding. \n",
    "\n",
    "WARNING: I'm using my desktop computer which is calibrated for CUDA processing. The timing printed below, from the Keras CNN \n",
    "processing, will vary depending on your hardware specification. I've printed my GPU specs below. A CPU will take 10 times longers generally.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3228522905\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1158385534894545445\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = list(device_lib.list_local_devices())\n",
    "print(device[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 500, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 3,964,271\n",
      "Trainable params: 3,964,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Without pretrained embedding, we just initalize the matrixs as:\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM)\n",
    "\n",
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# Use 1 Convolution Kernal \n",
    "model.add(e)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))  # 7 targets, each done as a logistic  \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup a checkpoint to ensure we save the best solution, and an early stopping procedure. And run the model for just 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9160Epoch 00000: val_acc improved from inf to 0.94372, saving model to weights_base.CovNet.hdf5\n",
      "64342/64342 [==============================] - 120s - loss: 0.2256 - acc: 0.9160 - val_loss: 0.1410 - val_acc: 0.9437\n",
      "Epoch 2/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9509Epoch 00001: val_acc did not improve\n",
      "64342/64342 [==============================] - 120s - loss: 0.1318 - acc: 0.9509 - val_loss: 0.1226 - val_acc: 0.9474\n",
      "Epoch 3/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9570Epoch 00002: val_acc did not improve\n",
      "64342/64342 [==============================] - 120s - loss: 0.1099 - acc: 0.9570 - val_loss: 0.1121 - val_acc: 0.9547\n",
      "Epoch 4/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9649Epoch 00003: val_acc did not improve\n",
      "64342/64342 [==============================] - 120s - loss: 0.0918 - acc: 0.9649 - val_loss: 0.1032 - val_acc: 0.9632\n",
      "Epoch 5/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9709Epoch 00004: val_acc did not improve\n",
      "64342/64342 [==============================] - 119s - loss: 0.0788 - acc: 0.9709 - val_loss: 0.0978 - val_acc: 0.9670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2572378f198>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.CovNet.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97%? Is that real? Thats a massive jump in accuracy over our best baseline which is at 86%. Well, note that the accuracy here is based on an Keras internal validation split. We'll need to test it on our own validation set that we split off earlier on. Each epoch took 2 mins to train. And seems like running it for 1 epoch is enough. Now lets see if the model overfitted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 5s     \n",
      "0.949498771726\n"
     ]
    }
   ],
   "source": [
    "# Load the model from epoch 1, which is the best. If we use the latest model from Epoch 5, accuracy is terrible. Guess why?\n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% accuracy! Trully impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN - With pretrained word embedding - 96% accuracy\n",
    "Now lets see how we what happens when we initialse using our pretrain word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 3,964,271\n",
      "Trainable params: 3,964,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding matrix we built from part 2. \n",
    "embedding_matrix = pickle.load(open(\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\embedding matrix.pickle\",\"rb\"))\n",
    "\n",
    "# CNN with initialised with the embedding matrix weights \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=seq_len, trainable=True) # trainable=True\n",
    "\n",
    "# Use 1 Convolution Kernal \n",
    "model.add(e)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save deal as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9228Epoch 00000: val_acc improved from inf to 0.95482, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CovNet_GloVe.hdf5\n",
      "64342/64342 [==============================] - 205s - loss: 0.2067 - acc: 0.9228 - val_loss: 0.1219 - val_acc: 0.9548\n",
      "Epoch 2/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9575Epoch 00001: val_acc did not improve\n",
      "64342/64342 [==============================] - 206s - loss: 0.1184 - acc: 0.9575 - val_loss: 0.0973 - val_acc: 0.9634\n",
      "Epoch 3/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9658Epoch 00002: val_acc did not improve\n",
      "64342/64342 [==============================] - 205s - loss: 0.0958 - acc: 0.9658 - val_loss: 0.0884 - val_acc: 0.9672\n",
      "Epoch 4/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9706Epoch 00003: val_acc did not improve\n",
      "64342/64342 [==============================] - 205s - loss: 0.0820 - acc: 0.9706 - val_loss: 0.0825 - val_acc: 0.9687\n",
      "Epoch 5/5\n",
      "64320/64342 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9741Epoch 00004: val_acc did not improve\n",
      "64342/64342 [==============================] - 207s - loss: 0.0718 - acc: 0.9741 - val_loss: 0.0799 - val_acc: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25723f50fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.CovNet_GloVe.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"min\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 2s     \n",
      "0.958152471437\n"
     ]
    }
   ],
   "source": [
    "# Load the model from epoch 1, which is the best. \n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96% accuracy! Pretty much the same as our CNN model without pretrain word embeddings. In all honesty, this turned out to be abit of a surpise for me. I expected CNN with pretrained word embedding to outperform a CNN without one, but I suppose due to its high accuracy already, the gains become really hard. \n",
    "\n",
    "At the end of the day, the winner here is CNN. Now you know why Deep Learning is the talk of the town"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
