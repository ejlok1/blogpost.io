{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "# A picture spoken with 1000 words, reported by CNN - Part 1 \n",
    "Eu Jin Lok\n",
    "\n",
    "9 February 2018\n",
    "\n",
    "# Establishing the benchmarks\n",
    "In this notebook we will go into the details of how to build a document classifier using CNN, a deep learning architecture well known for images classification. For the full background on this topic, please checkout my blog post in this link: \n",
    "\n",
    "xxxxxxxxxxx\n",
    "\n",
    "This is part 1 of the code which looks to establish some simple benchmark. We will be using the \"HappyDB\" dataset from Kaggle for our experiment: \n",
    "\n",
    "https://www.kaggle.com/ritresearch/happydb\n",
    "\n",
    "So without further ado, lets begin...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [],
   "source": [
    "#import the key libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import crosstab\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\CNN\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first step after loadings the necessary packages, we'll go grab our training dataset. This time around I'll be using the \"HappyDB\" dataset on Kaggle for our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100535, 9)\n",
      "hmid                         0\n",
      "wid                          0\n",
      "reflection_period            0\n",
      "original_hm                  0\n",
      "cleaned_hm                   0\n",
      "modified                     0\n",
      "num_sentence                 0\n",
      "ground_truth_category    86410\n",
      "predicted_category           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "train = pd.read_csv(\"happydb\\\\cleaned_hm.csv\")  \n",
    "\n",
    "# run some checks \n",
    "train.head(3)\n",
    "print(train.shape)\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'achievement': 4, 'nature': 6, 'leisure': 3, 'bonding': 2, 'exercise': 1, 'affection': 0, 'enjoy_the_moment': 5}\n"
     ]
    }
   ],
   "source": [
    "# Lets one-hot encode the labels  \n",
    "labels=train.predicted_category.unique()\n",
    "dic={}\n",
    "for i,labels in enumerate(labels):\n",
    "    dic[labels]=i\n",
    "labels=train.predicted_category.apply(lambda x:dic[x])\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the dataset, creating a reference dictionary of labels and their associated IDs, we'll split the dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(train.cleaned_hm, labels, test_size=0.20)\n",
    "\n",
    "#pull the data into vectors\n",
    "vectorizer = CountVectorizer(max_features=1000) #1000 Since our theme is a thousand words \n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "#Apply the vectoriser on test data using the previous vocabulary set \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "cvec_t = CountVectorizer(vocabulary=feature_names)\n",
    "x_test = cvec_t.fit_transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 1: Multinomial Naive Bayes = 82%\n",
    "So before we dive straight into CNN, lets establish some simple models first so we have something to benchmark against. First one of the list Multinomial NB. Why? Easy to code and no tuning necessary to get a resonable result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82384244293032283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "nb.score(x_test, y_test) #the test dataset is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82% accuracy which is pretty good for a simple Multinomial Naive Bayes based on top 1000 words. Lets see how a Tree Ensemble stacks up... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 2: Tree Ensemble = 84%\n",
    "Next on the list is Tree Ensemble, aka RandomForest. 4 years ago, this model used to be the most popular off-the-shelf model that everyone goes to to quickly gauge whether the dataset has enough signal in it, or maybe it needs reworking (more data cleaning to purge out the noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=50, verbose=1,n_jobs =-1)\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84129905008206096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.score(x_test, y_test) #the test dataset is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% accuracy, only slightly better over the Bayes model. Lets try another model that in recent times has gain abit of popularity, especially amongst folks in Kaggle..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 3: Gradient Boosting = 86%\n",
    "Its called the LightGBM. An implementation of the Gradient Boosting architecture, and is very similar to the XGboost but faster apparently. I have not used it before and I've always wanted to try them... and I will! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's multi_error: 0.227433\tvalid_0's multi_logloss: 1.85111\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's multi_error: 0.233252\tvalid_0's multi_logloss: 1.77402\n",
      "[3]\tvalid_0's multi_error: 0.230616\tvalid_0's multi_logloss: 1.70162\n",
      "[4]\tvalid_0's multi_error: 0.235789\tvalid_0's multi_logloss: 1.63935\n",
      "[5]\tvalid_0's multi_error: 0.232357\tvalid_0's multi_logloss: 1.58004\n",
      "[6]\tvalid_0's multi_error: 0.229373\tvalid_0's multi_logloss: 1.52626\n",
      "[7]\tvalid_0's multi_error: 0.226041\tvalid_0's multi_logloss: 1.47691\n",
      "[8]\tvalid_0's multi_error: 0.228577\tvalid_0's multi_logloss: 1.43151\n",
      "[9]\tvalid_0's multi_error: 0.229174\tvalid_0's multi_logloss: 1.38932\n",
      "[10]\tvalid_0's multi_error: 0.229423\tvalid_0's multi_logloss: 1.35012\n",
      "[11]\tvalid_0's multi_error: 0.227185\tvalid_0's multi_logloss: 1.3135\n",
      "[12]\tvalid_0's multi_error: 0.225643\tvalid_0's multi_logloss: 1.27949\n",
      "[13]\tvalid_0's multi_error: 0.225593\tvalid_0's multi_logloss: 1.24937\n",
      "[14]\tvalid_0's multi_error: 0.224449\tvalid_0's multi_logloss: 1.21789\n",
      "[15]\tvalid_0's multi_error: 0.223952\tvalid_0's multi_logloss: 1.18925\n",
      "[16]\tvalid_0's multi_error: 0.224002\tvalid_0's multi_logloss: 1.16223\n",
      "[17]\tvalid_0's multi_error: 0.221614\tvalid_0's multi_logloss: 1.13735\n",
      "[18]\tvalid_0's multi_error: 0.221515\tvalid_0's multi_logloss: 1.11409\n",
      "[19]\tvalid_0's multi_error: 0.221316\tvalid_0's multi_logloss: 1.09149\n",
      "[20]\tvalid_0's multi_error: 0.219675\tvalid_0's multi_logloss: 1.07001\n",
      "[21]\tvalid_0's multi_error: 0.219128\tvalid_0's multi_logloss: 1.04971\n",
      "[22]\tvalid_0's multi_error: 0.218531\tvalid_0's multi_logloss: 1.03059\n",
      "[23]\tvalid_0's multi_error: 0.218133\tvalid_0's multi_logloss: 1.0132\n",
      "[24]\tvalid_0's multi_error: 0.216641\tvalid_0's multi_logloss: 0.994694\n",
      "[25]\tvalid_0's multi_error: 0.216144\tvalid_0's multi_logloss: 0.977315\n",
      "[26]\tvalid_0's multi_error: 0.215447\tvalid_0's multi_logloss: 0.96035\n",
      "[27]\tvalid_0's multi_error: 0.214552\tvalid_0's multi_logloss: 0.943776\n",
      "[28]\tvalid_0's multi_error: 0.212911\tvalid_0's multi_logloss: 0.928429\n",
      "[29]\tvalid_0's multi_error: 0.212165\tvalid_0's multi_logloss: 0.913255\n",
      "[30]\tvalid_0's multi_error: 0.211618\tvalid_0's multi_logloss: 0.898516\n",
      "[31]\tvalid_0's multi_error: 0.210673\tvalid_0's multi_logloss: 0.885402\n",
      "[32]\tvalid_0's multi_error: 0.210225\tvalid_0's multi_logloss: 0.872094\n",
      "[33]\tvalid_0's multi_error: 0.209579\tvalid_0's multi_logloss: 0.859568\n",
      "[34]\tvalid_0's multi_error: 0.208882\tvalid_0's multi_logloss: 0.847995\n",
      "[35]\tvalid_0's multi_error: 0.208286\tvalid_0's multi_logloss: 0.836851\n",
      "[36]\tvalid_0's multi_error: 0.207341\tvalid_0's multi_logloss: 0.825301\n",
      "[37]\tvalid_0's multi_error: 0.205998\tvalid_0's multi_logloss: 0.814396\n",
      "[38]\tvalid_0's multi_error: 0.205053\tvalid_0's multi_logloss: 0.803823\n",
      "[39]\tvalid_0's multi_error: 0.204158\tvalid_0's multi_logloss: 0.793962\n",
      "[40]\tvalid_0's multi_error: 0.203113\tvalid_0's multi_logloss: 0.784996\n",
      "[41]\tvalid_0's multi_error: 0.202318\tvalid_0's multi_logloss: 0.775522\n",
      "[42]\tvalid_0's multi_error: 0.201621\tvalid_0's multi_logloss: 0.766507\n",
      "[43]\tvalid_0's multi_error: 0.200925\tvalid_0's multi_logloss: 0.757933\n",
      "[44]\tvalid_0's multi_error: 0.200776\tvalid_0's multi_logloss: 0.749574\n",
      "[45]\tvalid_0's multi_error: 0.200627\tvalid_0's multi_logloss: 0.74187\n",
      "[46]\tvalid_0's multi_error: 0.199284\tvalid_0's multi_logloss: 0.734257\n",
      "[47]\tvalid_0's multi_error: 0.199483\tvalid_0's multi_logloss: 0.72744\n",
      "[48]\tvalid_0's multi_error: 0.19819\tvalid_0's multi_logloss: 0.7206\n",
      "[49]\tvalid_0's multi_error: 0.197643\tvalid_0's multi_logloss: 0.713411\n",
      "[50]\tvalid_0's multi_error: 0.197145\tvalid_0's multi_logloss: 0.707033\n",
      "[51]\tvalid_0's multi_error: 0.19635\tvalid_0's multi_logloss: 0.70034\n",
      "[52]\tvalid_0's multi_error: 0.196151\tvalid_0's multi_logloss: 0.693817\n",
      "[53]\tvalid_0's multi_error: 0.194659\tvalid_0's multi_logloss: 0.687735\n",
      "[54]\tvalid_0's multi_error: 0.194062\tvalid_0's multi_logloss: 0.681615\n",
      "[55]\tvalid_0's multi_error: 0.193465\tvalid_0's multi_logloss: 0.67603\n",
      "[56]\tvalid_0's multi_error: 0.192669\tvalid_0's multi_logloss: 0.670169\n",
      "[57]\tvalid_0's multi_error: 0.192023\tvalid_0's multi_logloss: 0.664975\n",
      "[58]\tvalid_0's multi_error: 0.191426\tvalid_0's multi_logloss: 0.659862\n",
      "[59]\tvalid_0's multi_error: 0.190332\tvalid_0's multi_logloss: 0.654715\n",
      "[60]\tvalid_0's multi_error: 0.189188\tvalid_0's multi_logloss: 0.6496\n",
      "[61]\tvalid_0's multi_error: 0.189088\tvalid_0's multi_logloss: 0.64489\n",
      "[62]\tvalid_0's multi_error: 0.188392\tvalid_0's multi_logloss: 0.640126\n",
      "[63]\tvalid_0's multi_error: 0.187397\tvalid_0's multi_logloss: 0.635605\n",
      "[64]\tvalid_0's multi_error: 0.187049\tvalid_0's multi_logloss: 0.631148\n",
      "[65]\tvalid_0's multi_error: 0.1869\tvalid_0's multi_logloss: 0.626623\n",
      "[66]\tvalid_0's multi_error: 0.186055\tvalid_0's multi_logloss: 0.622432\n",
      "[67]\tvalid_0's multi_error: 0.185905\tvalid_0's multi_logloss: 0.618616\n",
      "[68]\tvalid_0's multi_error: 0.185657\tvalid_0's multi_logloss: 0.614737\n",
      "[69]\tvalid_0's multi_error: 0.185607\tvalid_0's multi_logloss: 0.611113\n",
      "[70]\tvalid_0's multi_error: 0.18506\tvalid_0's multi_logloss: 0.60732\n",
      "[71]\tvalid_0's multi_error: 0.184364\tvalid_0's multi_logloss: 0.603381\n",
      "[72]\tvalid_0's multi_error: 0.183817\tvalid_0's multi_logloss: 0.599684\n",
      "[73]\tvalid_0's multi_error: 0.183319\tvalid_0's multi_logloss: 0.596285\n",
      "[74]\tvalid_0's multi_error: 0.182872\tvalid_0's multi_logloss: 0.592852\n",
      "[75]\tvalid_0's multi_error: 0.181927\tvalid_0's multi_logloss: 0.589397\n",
      "[76]\tvalid_0's multi_error: 0.181429\tvalid_0's multi_logloss: 0.585923\n",
      "[77]\tvalid_0's multi_error: 0.180584\tvalid_0's multi_logloss: 0.582715\n",
      "[78]\tvalid_0's multi_error: 0.17939\tvalid_0's multi_logloss: 0.579391\n",
      "[79]\tvalid_0's multi_error: 0.178992\tvalid_0's multi_logloss: 0.576278\n",
      "[80]\tvalid_0's multi_error: 0.178246\tvalid_0's multi_logloss: 0.573102\n",
      "[81]\tvalid_0's multi_error: 0.178097\tvalid_0's multi_logloss: 0.56957\n",
      "[82]\tvalid_0's multi_error: 0.177351\tvalid_0's multi_logloss: 0.566551\n",
      "[83]\tvalid_0's multi_error: 0.176456\tvalid_0's multi_logloss: 0.56378\n",
      "[84]\tvalid_0's multi_error: 0.176207\tvalid_0's multi_logloss: 0.560525\n",
      "[85]\tvalid_0's multi_error: 0.175213\tvalid_0's multi_logloss: 0.557329\n",
      "[86]\tvalid_0's multi_error: 0.174765\tvalid_0's multi_logloss: 0.554616\n",
      "[87]\tvalid_0's multi_error: 0.174268\tvalid_0's multi_logloss: 0.552103\n",
      "[88]\tvalid_0's multi_error: 0.173323\tvalid_0's multi_logloss: 0.549527\n",
      "[89]\tvalid_0's multi_error: 0.172875\tvalid_0's multi_logloss: 0.546942\n",
      "[90]\tvalid_0's multi_error: 0.172875\tvalid_0's multi_logloss: 0.544181\n",
      "[91]\tvalid_0's multi_error: 0.172527\tvalid_0's multi_logloss: 0.541464\n",
      "[92]\tvalid_0's multi_error: 0.172378\tvalid_0's multi_logloss: 0.53879\n",
      "[93]\tvalid_0's multi_error: 0.17193\tvalid_0's multi_logloss: 0.536114\n",
      "[94]\tvalid_0's multi_error: 0.171234\tvalid_0's multi_logloss: 0.533721\n",
      "[95]\tvalid_0's multi_error: 0.171333\tvalid_0's multi_logloss: 0.531539\n",
      "[96]\tvalid_0's multi_error: 0.170886\tvalid_0's multi_logloss: 0.529052\n",
      "[97]\tvalid_0's multi_error: 0.170289\tvalid_0's multi_logloss: 0.526549\n",
      "[98]\tvalid_0's multi_error: 0.169891\tvalid_0's multi_logloss: 0.524147\n",
      "[99]\tvalid_0's multi_error: 0.169642\tvalid_0's multi_logloss: 0.52211\n",
      "[100]\tvalid_0's multi_error: 0.169145\tvalid_0's multi_logloss: 0.51992\n",
      "[101]\tvalid_0's multi_error: 0.168548\tvalid_0's multi_logloss: 0.517749\n",
      "[102]\tvalid_0's multi_error: 0.167703\tvalid_0's multi_logloss: 0.515844\n",
      "[103]\tvalid_0's multi_error: 0.167305\tvalid_0's multi_logloss: 0.513917\n",
      "[104]\tvalid_0's multi_error: 0.167255\tvalid_0's multi_logloss: 0.512207\n",
      "[105]\tvalid_0's multi_error: 0.166907\tvalid_0's multi_logloss: 0.510385\n",
      "[106]\tvalid_0's multi_error: 0.166111\tvalid_0's multi_logloss: 0.508147\n",
      "[107]\tvalid_0's multi_error: 0.165962\tvalid_0's multi_logloss: 0.50596\n",
      "[108]\tvalid_0's multi_error: 0.165614\tvalid_0's multi_logloss: 0.503893\n",
      "[109]\tvalid_0's multi_error: 0.165017\tvalid_0's multi_logloss: 0.501916\n",
      "[110]\tvalid_0's multi_error: 0.16452\tvalid_0's multi_logloss: 0.499973\n",
      "[111]\tvalid_0's multi_error: 0.163426\tvalid_0's multi_logloss: 0.498324\n",
      "[112]\tvalid_0's multi_error: 0.163078\tvalid_0's multi_logloss: 0.496392\n",
      "[113]\tvalid_0's multi_error: 0.162829\tvalid_0's multi_logloss: 0.494762\n",
      "[114]\tvalid_0's multi_error: 0.162729\tvalid_0's multi_logloss: 0.493157\n",
      "[115]\tvalid_0's multi_error: 0.162232\tvalid_0's multi_logloss: 0.491397\n",
      "[116]\tvalid_0's multi_error: 0.162033\tvalid_0's multi_logloss: 0.489717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117]\tvalid_0's multi_error: 0.161834\tvalid_0's multi_logloss: 0.488042\n",
      "[118]\tvalid_0's multi_error: 0.161287\tvalid_0's multi_logloss: 0.486329\n",
      "[119]\tvalid_0's multi_error: 0.16079\tvalid_0's multi_logloss: 0.484638\n",
      "[120]\tvalid_0's multi_error: 0.161038\tvalid_0's multi_logloss: 0.483141\n",
      "[121]\tvalid_0's multi_error: 0.160243\tvalid_0's multi_logloss: 0.481544\n",
      "[122]\tvalid_0's multi_error: 0.160292\tvalid_0's multi_logloss: 0.480109\n",
      "[123]\tvalid_0's multi_error: 0.160044\tvalid_0's multi_logloss: 0.478573\n",
      "[124]\tvalid_0's multi_error: 0.159745\tvalid_0's multi_logloss: 0.477087\n",
      "[125]\tvalid_0's multi_error: 0.159248\tvalid_0's multi_logloss: 0.475685\n",
      "[126]\tvalid_0's multi_error: 0.159149\tvalid_0's multi_logloss: 0.474334\n",
      "[127]\tvalid_0's multi_error: 0.1588\tvalid_0's multi_logloss: 0.473097\n",
      "[128]\tvalid_0's multi_error: 0.1589\tvalid_0's multi_logloss: 0.471863\n",
      "[129]\tvalid_0's multi_error: 0.158552\tvalid_0's multi_logloss: 0.470629\n",
      "[130]\tvalid_0's multi_error: 0.158651\tvalid_0's multi_logloss: 0.469394\n",
      "[131]\tvalid_0's multi_error: 0.158253\tvalid_0's multi_logloss: 0.468092\n",
      "[132]\tvalid_0's multi_error: 0.157855\tvalid_0's multi_logloss: 0.466793\n",
      "[133]\tvalid_0's multi_error: 0.157458\tvalid_0's multi_logloss: 0.465483\n",
      "[134]\tvalid_0's multi_error: 0.156811\tvalid_0's multi_logloss: 0.46427\n",
      "[135]\tvalid_0's multi_error: 0.156811\tvalid_0's multi_logloss: 0.463086\n",
      "[136]\tvalid_0's multi_error: 0.156513\tvalid_0's multi_logloss: 0.46186\n",
      "[137]\tvalid_0's multi_error: 0.156015\tvalid_0's multi_logloss: 0.460619\n",
      "[138]\tvalid_0's multi_error: 0.155518\tvalid_0's multi_logloss: 0.459314\n",
      "[139]\tvalid_0's multi_error: 0.15522\tvalid_0's multi_logloss: 0.458135\n",
      "[140]\tvalid_0's multi_error: 0.154772\tvalid_0's multi_logloss: 0.457013\n",
      "[141]\tvalid_0's multi_error: 0.154971\tvalid_0's multi_logloss: 0.455809\n",
      "[142]\tvalid_0's multi_error: 0.154673\tvalid_0's multi_logloss: 0.454715\n",
      "[143]\tvalid_0's multi_error: 0.154523\tvalid_0's multi_logloss: 0.453571\n",
      "[144]\tvalid_0's multi_error: 0.154523\tvalid_0's multi_logloss: 0.45263\n",
      "[145]\tvalid_0's multi_error: 0.154026\tvalid_0's multi_logloss: 0.451573\n",
      "[146]\tvalid_0's multi_error: 0.153529\tvalid_0's multi_logloss: 0.450311\n",
      "[147]\tvalid_0's multi_error: 0.153578\tvalid_0's multi_logloss: 0.449167\n",
      "[148]\tvalid_0's multi_error: 0.15328\tvalid_0's multi_logloss: 0.447993\n",
      "[149]\tvalid_0's multi_error: 0.152683\tvalid_0's multi_logloss: 0.446928\n",
      "[150]\tvalid_0's multi_error: 0.152285\tvalid_0's multi_logloss: 0.445754\n",
      "[151]\tvalid_0's multi_error: 0.15149\tvalid_0's multi_logloss: 0.444671\n",
      "[152]\tvalid_0's multi_error: 0.151589\tvalid_0's multi_logloss: 0.443531\n",
      "[153]\tvalid_0's multi_error: 0.151092\tvalid_0's multi_logloss: 0.442446\n",
      "[154]\tvalid_0's multi_error: 0.150545\tvalid_0's multi_logloss: 0.441515\n",
      "[155]\tvalid_0's multi_error: 0.150694\tvalid_0's multi_logloss: 0.440493\n",
      "[156]\tvalid_0's multi_error: 0.150793\tvalid_0's multi_logloss: 0.439534\n",
      "[157]\tvalid_0's multi_error: 0.150694\tvalid_0's multi_logloss: 0.43858\n",
      "[158]\tvalid_0's multi_error: 0.150495\tvalid_0's multi_logloss: 0.437642\n",
      "[159]\tvalid_0's multi_error: 0.150346\tvalid_0's multi_logloss: 0.43676\n",
      "[160]\tvalid_0's multi_error: 0.149998\tvalid_0's multi_logloss: 0.435919\n",
      "[161]\tvalid_0's multi_error: 0.149749\tvalid_0's multi_logloss: 0.434989\n",
      "[162]\tvalid_0's multi_error: 0.149649\tvalid_0's multi_logloss: 0.434101\n",
      "[163]\tvalid_0's multi_error: 0.1495\tvalid_0's multi_logloss: 0.433198\n",
      "[164]\tvalid_0's multi_error: 0.148903\tvalid_0's multi_logloss: 0.432282\n",
      "[165]\tvalid_0's multi_error: 0.148605\tvalid_0's multi_logloss: 0.431491\n",
      "[166]\tvalid_0's multi_error: 0.148555\tvalid_0's multi_logloss: 0.430634\n",
      "[167]\tvalid_0's multi_error: 0.148356\tvalid_0's multi_logloss: 0.429753\n",
      "[168]\tvalid_0's multi_error: 0.148108\tvalid_0's multi_logloss: 0.428895\n",
      "[169]\tvalid_0's multi_error: 0.147958\tvalid_0's multi_logloss: 0.428057\n",
      "[170]\tvalid_0's multi_error: 0.147511\tvalid_0's multi_logloss: 0.427234\n",
      "[171]\tvalid_0's multi_error: 0.147163\tvalid_0's multi_logloss: 0.426346\n",
      "[172]\tvalid_0's multi_error: 0.147113\tvalid_0's multi_logloss: 0.425496\n",
      "[173]\tvalid_0's multi_error: 0.146864\tvalid_0's multi_logloss: 0.424634\n",
      "[174]\tvalid_0's multi_error: 0.146566\tvalid_0's multi_logloss: 0.423797\n",
      "[175]\tvalid_0's multi_error: 0.145969\tvalid_0's multi_logloss: 0.423007\n",
      "[176]\tvalid_0's multi_error: 0.146069\tvalid_0's multi_logloss: 0.422225\n",
      "[177]\tvalid_0's multi_error: 0.145571\tvalid_0's multi_logloss: 0.421424\n",
      "[178]\tvalid_0's multi_error: 0.145422\tvalid_0's multi_logloss: 0.420646\n",
      "[179]\tvalid_0's multi_error: 0.144875\tvalid_0's multi_logloss: 0.419908\n",
      "[180]\tvalid_0's multi_error: 0.144577\tvalid_0's multi_logloss: 0.419164\n",
      "[181]\tvalid_0's multi_error: 0.144328\tvalid_0's multi_logloss: 0.418354\n",
      "[182]\tvalid_0's multi_error: 0.144726\tvalid_0's multi_logloss: 0.417521\n",
      "[183]\tvalid_0's multi_error: 0.144079\tvalid_0's multi_logloss: 0.416777\n",
      "[184]\tvalid_0's multi_error: 0.144029\tvalid_0's multi_logloss: 0.416078\n",
      "[185]\tvalid_0's multi_error: 0.143831\tvalid_0's multi_logloss: 0.415337\n",
      "[186]\tvalid_0's multi_error: 0.143532\tvalid_0's multi_logloss: 0.414675\n",
      "[187]\tvalid_0's multi_error: 0.143383\tvalid_0's multi_logloss: 0.413986\n",
      "[188]\tvalid_0's multi_error: 0.143532\tvalid_0's multi_logloss: 0.413303\n",
      "[189]\tvalid_0's multi_error: 0.143433\tvalid_0's multi_logloss: 0.412556\n",
      "[190]\tvalid_0's multi_error: 0.142935\tvalid_0's multi_logloss: 0.411837\n",
      "[191]\tvalid_0's multi_error: 0.142587\tvalid_0's multi_logloss: 0.411172\n",
      "[192]\tvalid_0's multi_error: 0.142338\tvalid_0's multi_logloss: 0.410519\n",
      "[193]\tvalid_0's multi_error: 0.141941\tvalid_0's multi_logloss: 0.409863\n",
      "[194]\tvalid_0's multi_error: 0.141941\tvalid_0's multi_logloss: 0.409199\n",
      "[195]\tvalid_0's multi_error: 0.14199\tvalid_0's multi_logloss: 0.408549\n",
      "[196]\tvalid_0's multi_error: 0.141692\tvalid_0's multi_logloss: 0.407831\n",
      "[197]\tvalid_0's multi_error: 0.141294\tvalid_0's multi_logloss: 0.40722\n",
      "[198]\tvalid_0's multi_error: 0.141244\tvalid_0's multi_logloss: 0.406567\n",
      "[199]\tvalid_0's multi_error: 0.141045\tvalid_0's multi_logloss: 0.405921\n",
      "[200]\tvalid_0's multi_error: 0.140996\tvalid_0's multi_logloss: 0.405382\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's multi_error: 0.140996\tvalid_0's multi_logloss: 0.405382\n"
     ]
    }
   ],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(x_train.astype(np.float64), y_train)\n",
    "lgb_eval = lgb.Dataset(x_test.astype(np.float64), y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'metric': {'multi_logloss', 'multi_error'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "    ,'num_class': 7\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=200,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85900432685134531"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = gbm.predict(x_test.astype(np.float64), num_iteration=gbm.best_iteration)\n",
    "pred = pd.DataFrame(pred).idxmax(axis=1)\n",
    "accuracy_score(pred, y_test) #the test dataset is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86% accuracy, only slightly better than Random Forest, but certainly has alot of room to improve if we tweak the parameters. But one thing notable with LightGBM, its blazing fast! \n",
    "\n",
    "Before we move on lets check the predictions against the actuals visually to make sure we haven't lost the plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'achievement': 4, 'nature': 6, 'leisure': 3, 'bonding': 2, 'exercise': 1, 'affection': 0, 'enjoy_the_moment': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_category</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6292</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>36</td>\n",
       "      <td>267</td>\n",
       "      <td>113</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1092</td>\n",
       "      <td>104</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>297</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>273</td>\n",
       "      <td>6205</td>\n",
       "      <td>632</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>203</td>\n",
       "      <td>1335</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_category     0    1     2     3     4     5    6\n",
       "row_0                                                     \n",
       "0                   6292    7    84    36   267   113   21\n",
       "1                      8  153     0     3    24     8    0\n",
       "2                     64    7  1965     5    49    17    3\n",
       "3                     21   11     2  1092   104    95   18\n",
       "4                    297   52    55   273  6205   632   61\n",
       "5                     55    7    13   108   203  1335   23\n",
       "6                     10    6     3    14    32    24  230"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dic)\n",
    "crosstab(pred, y_test.reset_index(drop=True))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
