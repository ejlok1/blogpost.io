{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "# Episode 6: Building castles in the sky, or a memory palace Part 2\n",
    "Eu Jin Lok\n",
    "\n",
    "8 April 2018\n",
    "\n",
    "# GRUs  \n",
    "In this notebook we will go into the details of how to build a document classifier using GRUs, a deep learning architecture that is able to remember long-term dependencies just like LSTM, but a much more simpler architecture. For the full background on this topic, please checkout my blog post in this link: \n",
    "\n",
    "https://mungingdata.wordpress.com/2018/04/13/episode-6-grus/\n",
    "\n",
    "This dataset is based on Episode 4 and builds upon the previous CNN architecture. So without further ado, lets begin...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the key libraries \n",
    "import pandas as pd \n",
    "from pandas import crosstab\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, MaxPooling1D, SpatialDropout1D, Dropout,Convolution1D, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\DONE CNN\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first step after loadings the necessary packages, we'll go grab our training dataset, the same one from the previous Episode 4, and I just copy the code here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23313 unique tokens or words.\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "train = pd.read_csv(\"happydb\\\\cleaned_hm.csv\")  \n",
    "\n",
    "# Lets one-hot encode the labels  \n",
    "labels=train.predicted_category.unique()\n",
    "dic={}\n",
    "for i,labels in enumerate(labels):\n",
    "    dic[labels]=i\n",
    "labels=train.predicted_category.apply(lambda x:dic[x])\n",
    "\n",
    "val=train.sample(frac=0.2,random_state=200)\n",
    "train=train.drop(val.index)\n",
    "\n",
    "NUM_WORDS=20000 # if set, tokenization will be restricted to the top num_words most common words in the dataset).\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "\n",
    "# we need to fit the tokenizer on our text data in order to get the tokens\n",
    "texts=train.cleaned_hm\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Found %s unique tokens or words.' % len(word_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the words the sentences in our documents to the index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a successful date with someone I felt sympathy and connection with.\n",
      "[1, 23, 16, 3, 758, 315, 13, 284, 1, 94, 9298, 5, 2393, 13]\n",
      "315 = index for the word 'Date' \n",
      "Date is an index number of 315. And it appears in the right position (5th) in the sentence\n"
     ]
    }
   ],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(texts) # converts the text to numbers essentially\n",
    "sequences_valid=tokenizer.texts_to_sequences(val.cleaned_hm)\n",
    "word_index = tokenizer.word_index\n",
    "#Although word_index contains all words tokenizer.texts_to_sequences takes num_words into account.\n",
    "\n",
    "# Check the index is working correctly \n",
    "print(texts[0])\n",
    "print(sequences_train[0])\n",
    "print(word_index['date'],\"= index for the word 'Date' \") \n",
    "print('Date is an index number of 315. And it appears in the right position (5th) in the sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting the data into an LSTM architecture, and we need to ensure the shape of the dataset is the same across all text. But because each text varies in lenght, we'll cap it at a fixed length, and just pad it with zeros to fill in the gaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1   23   16    3\n",
      "  758  315   13  284    1   94 9298    5 2393   13]\n"
     ]
    }
   ],
   "source": [
    "# set the sequence length of the text to speed up training and prevent overfitting. \n",
    "seq_len = 500\n",
    "X_train = pad_sequences(sequences_train,maxlen=seq_len, value=0)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=seq_len, value=0)\n",
    "\n",
    "# Lets check a single record to see how it looks\n",
    "print(X_train[0]) # By default we pad the left side. In order words, all the text is right side aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last part of the processing is to one-hot encode / binarise the target. That's the format that works well with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80428, 7)\n",
      "(20107, 7)\n"
     ]
    }
   ],
   "source": [
    "y_train =train.predicted_category.apply(lambda x:dic[x])\n",
    "y_train = to_categorical(np.asarray(labels[train.index]))\n",
    "y_val =val.predicted_category.apply(lambda x:dic[x])\n",
    "y_val = to_categorical(np.asarray(labels[y_val.index]))\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU = 97% accuracy\n",
    "And here we are. Since this notebook is going to be short, I thought I'll showcase a few variations of LSTM. Lets start with a simple LSTM without using a pretrained embedding. \n",
    "\n",
    "WARNING: I'm using my desktop computer which is calibrated for CUDA processing. The timing printed below, will vary depending on your hardware specification. I've printed my GPU specs below. A CPU will take 10 times longers generally... and LSTM takes a long long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3228522905\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 4948440754896512877\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = list(device_lib.list_local_devices())\n",
    "print(device[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               360900    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 3,007,699\n",
      "Trainable params: 3,007,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# Use a simple LSTM structure\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(7, activation='sigmoid'))  # 7 targets, each done as a logistic  \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup a checkpoint to ensure we save the best solution, and an early stopping procedure. And run the model for just 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 582s 9ms/step - loss: 0.1731 - acc: 0.9336 - val_loss: 0.0934 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96386, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 581s 9ms/step - loss: 0.0802 - acc: 0.9696 - val_loss: 0.0719 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96386 to 0.97153, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 583s 9ms/step - loss: 0.0583 - acc: 0.9780 - val_loss: 0.0669 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.97153 to 0.97306, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 581s 9ms/step - loss: 0.0465 - acc: 0.9827 - val_loss: 0.0667 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97306 to 0.97457, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 581s 9ms/step - loss: 0.0387 - acc: 0.9856 - val_loss: 0.0660 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97457 to 0.97536, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aaf8e590b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.GRU.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97%, and seems like it could continue to improve further... but lets stop at 5 epochs and you can try it yourself. Note that LSTM takes incrementally longer when compared to CNN. Now lets confirm the accuracy by applying it to the actual validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 92s 5ms/step\n",
      "0.9769732045197692\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the best accuracy\n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.7% accuracy! This is our best model so far! Now lets see how we what happens when we use a hybrid model of CNN and LSTM..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and GRU layers = 97%\n",
    "Start with the standard embedding layer first, then followed by a Convolution layer followed by an GRU. A hybrid model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               49500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 2,423,771\n",
      "Trainable params: 2,423,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# Use a Convolution Kernal first then LSTM \n",
    "model.add(e)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup checkpoints..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 272s 4ms/step - loss: 0.1937 - acc: 0.9247 - val_loss: 0.1190 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95039, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_GRU.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 276s 4ms/step - loss: 0.1016 - acc: 0.9640 - val_loss: 0.0940 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95039 to 0.96498, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_GRU.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 276s 4ms/step - loss: 0.0756 - acc: 0.9736 - val_loss: 0.0906 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96498 to 0.96684, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_GRU.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 278s 4ms/step - loss: 0.0615 - acc: 0.9786 - val_loss: 0.0853 - val_acc: 0.9680\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96684 to 0.96796, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_GRU.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 277s 4ms/step - loss: 0.0511 - acc: 0.9821 - val_loss: 0.0887 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96796 to 0.96990, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_GRU.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2ffb3e358>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.CNN_GRU.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 39s 2ms/step\n",
      "0.9732360543584327\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the best accuracy\n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.3% accuracy! Pretty good but not as good as just a normal GRU. But could just be a lucky random seed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double GRU layers = 97%\n",
    "Now lets try a double GRU layer .... this might take awhile to finish..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 500, 100)          60300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 100)               60300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 2,452,707\n",
      "Trainable params: 2,452,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# After the embedding layer, use an LSTM and then another LSTM. First LSTM returns the sequence length as outputs \n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3,return_sequences = True))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 1016s 16ms/step - loss: 0.2184 - acc: 0.9169 - val_loss: 0.1357 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94480, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU_GRU.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 1048s 16ms/step - loss: 0.1294 - acc: 0.9514 - val_loss: 0.1178 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94480 to 0.95342, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU_GRU.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 997s 15ms/step - loss: 0.1055 - acc: 0.9636 - val_loss: 0.0963 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95342 to 0.96414, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU_GRU.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 985s 15ms/step - loss: 0.0897 - acc: 0.9696 - val_loss: 0.0867 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96414 to 0.96723, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU_GRU.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 991s 15ms/step - loss: 0.0778 - acc: 0.9730 - val_loss: 0.0830 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96723 to 0.96935, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.GRU_GRU.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e948688d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.GRU_GRU.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 153s 8ms/step\n",
      "0.9710193440601946\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the best accuracy\n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.1% accuracy! Pretty good but not as good as just a normal GRU. Again, could just be a lucky random seed "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
