{"cells": [{"cell_type": "markdown", "id": "adfb8c33-b6ea-4142-89da-f8b6de9d1351", "metadata": {"nbpresent": {"id": "42344751-df85-4fa9-af08-b4bf50b41e05"}}, "outputs": [], "source": ["# Code demo for Word Embeddings - Part 1 \n", "Eu Jin Lok\n", "\n", "10 January 2018\n", "\n", "# How does word embeddings add value to predictive models\n", "In this notebook we will go into the details of how to build your own word embeddings, and use it as a powerful feature to improve you predictive model. For the full background on this topic, please checkout my blog post in this link: \n", "\n", "https://mungingdata.wordpress.com/2018/01/15/episode-3-word-embeddings/\n", "\n", "This is part 1 of the code which replicates as close as possible the existing benchmark, see link:\n", "\n", "https://www.kaggle.com/kinguistics/classifying-news-headlines-with-scikit-learn\n", "\n", "So without further ado, lets begin.... oh and Happy New year 2018! "]}, {"cell_type": "code", "execution_count": 2, "id": "c5bb2812-6d23-4895-b8ec-510b30aca431", "metadata": {"collapsed": true, "nbpresent": {"id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"}}, "outputs": [], "source": ["#import the key libraries \n", "import re\n", "import numpy as np \n", "import pandas as pd \n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.preprocessing import LabelEncoder\n", "import os \n", "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\word embeddings\\\\\")\n", "np.random.seed(789)\n", "\n", "# Basic text processing function for the news group dataset \n", "def normalize_text(s):\n", "    s = s.lower()\n", "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n", "    s = re.sub('\\s\\W',' ',s)\n", "    s = re.sub('\\W\\s',' ',s)\n", "    # make sure we didn't introduce any double spaces\n", "    s = re.sub('\\s+',' ',s)\n", "    return s"]}, {"cell_type": "markdown", "id": "420ee85c-ae53-4a24-99cf-6f8d51709a14", "metadata": {}, "outputs": [], "source": ["So first step after loadings the necessary packages, lets go grab our training dataset, the news aggregator dataset"]}, {"cell_type": "code", "execution_count": 3, "id": "92f783cb-d26f-4da0-b3c9-9c2974a8ea7e", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style>\n", "    .dataframe thead tr:only-child th {\n", "        text-align: right;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: left;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>ID</th>\n", "      <th>TITLE</th>\n", "      <th>URL</th>\n", "      <th>PUBLISHER</th>\n", "      <th>CATEGORY</th>\n", "      <th>STORY</th>\n", "      <th>HOSTNAME</th>\n", "      <th>TIMESTAMP</th>\n", "      <th>TEXT</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1</td>\n", "      <td>Fed official says weak data caused by weather,...</td>\n", "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n", "      <td>Los Angeles Times</td>\n", "      <td>b</td>\n", "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n", "      <td>www.latimes.com</td>\n", "      <td>1394470370698</td>\n", "      <td>fed official says weak data caused by weather ...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2</td>\n", "      <td>Fed's Charles Plosser sees high bar for change...</td>\n", "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n", "      <td>Livemint</td>\n", "      <td>b</td>\n", "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n", "      <td>www.livemint.com</td>\n", "      <td>1394470371207</td>\n", "      <td>fed's charles plosser sees high bar for change...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>3</td>\n", "      <td>US open: Stocks fall after Fed official hints ...</td>\n", "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n", "      <td>IFA Magazine</td>\n", "      <td>b</td>\n", "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n", "      <td>www.ifamagazine.com</td>\n", "      <td>1394470371550</td>\n", "      <td>us open stocks fall after fed official hints a...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>4</td>\n", "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n", "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n", "      <td>IFA Magazine</td>\n", "      <td>b</td>\n", "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n", "      <td>www.ifamagazine.com</td>\n", "      <td>1394470371793</td>\n", "      <td>fed risks falling behind the curve' charles pl...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>5</td>\n", "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n", "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n", "      <td>Moneynews</td>\n", "      <td>b</td>\n", "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n", "      <td>www.moneynews.com</td>\n", "      <td>1394470372027</td>\n", "      <td>fed's plosser nasty weather has curbed job growth</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   ID                                              TITLE  \\\n", "0   1  Fed official says weak data caused by weather,...   \n", "1   2  Fed's Charles Plosser sees high bar for change...   \n", "2   3  US open: Stocks fall after Fed official hints ...   \n", "3   4  Fed risks falling 'behind the curve', Charles ...   \n", "4   5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n", "\n", "                                                 URL          PUBLISHER  \\\n", "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n", "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n", "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n", "3  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n", "4  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n", "\n", "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \\\n", "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698   \n", "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207   \n", "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550   \n", "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793   \n", "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027   \n", "\n", "                                                TEXT  \n", "0  fed official says weak data caused by weather ...  \n", "1  fed's charles plosser sees high bar for change...  \n", "2  us open stocks fall after fed official hints a...  \n", "3  fed risks falling behind the curve' charles pl...  \n", "4  fed's plosser nasty weather has curbed job growth  "]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["# Grab the data, Download from the Kaggle website\n", "news = pd.read_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\uci-news-aggregator.csv\")\n", "\n", "news['TEXT'] = [normalize_text(s) for s in news['TITLE']]\n", "news.head()"]}, {"cell_type": "markdown", "id": "1546b142-dfcd-4976-bcf5-30bd24384354", "metadata": {}, "outputs": [], "source": ["As per the blog, I'm following most of Ed King's code, with the exception of just 3 key components:\n", "\n", "1) Minimise the count vectoriser to just 300 features\n", "\n", "2) Build the vectoriser on just the Training dataset, and the test dataset is fitted on. This is good practice in real world scenarios \n", "\n", "3) Using n <= 500 as sample size for training\n", "\n", "Reasoning for the above is explained in the blog. So lets move on and extract the features using the Vectorizer in sklearn"]}, {"cell_type": "code", "execution_count": 4, "id": "b2eefe29-bcc8-4339-8dba-1a15d77daa7a", "metadata": {"collapsed": true}, "outputs": [], "source": ["#set the labels\n", "encoder = LabelEncoder()\n", "y = encoder.fit_transform(news['CATEGORY'])\n", "\n", "#split into train and test sets\n", "x_train, x_test, y_train, y_test = train_test_split(news['TEXT'], y, test_size=0.999)\n", "\n", "#pull the data into vectors\n", "vectorizer = CountVectorizer(max_features=300) \n", "x_train = vectorizer.fit_transform(x_train)\n", "\n", "#Apply the vectoriser on test data using the previous vocabulary set \n", "feature_names = vectorizer.get_feature_names()\n", "cvec_t = CountVectorizer(vocabulary=feature_names)\n", "x_test = cvec_t.fit_transform(x_test).toarray()"]}, {"cell_type": "markdown", "id": "7b5aa3d1-9ad4-4b81-bedd-b499b2a27998", "metadata": {}, "outputs": [], "source": ["Now run the Multinomial NaiveBayes model as per the original kernel from Ed King, to keep things as consistent as possible."]}, {"cell_type": "code", "execution_count": 5, "id": "36cf9297-7323-4d13-991a-760da1ae33e8", "metadata": {}, "outputs": [{"data": {"text/plain": ["0.59461323184761972"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["# We'll stick to the same model - Multinomial NB\n", "nb = MultinomialNB()\n", "nb.fit(x_train, y_train)\n", "nb.score(x_test, y_test) #the test dataset is "]}, {"cell_type": "markdown", "id": "7bf5e349-7d1c-4c78-8b09-9048f8bd1753", "metadata": {}, "outputs": [], "source": ["~0.58 accuracy just using 300 bag-of-words features, and with only 1% training data. That's pretty solid given the circumstances. So now lets go to the next part of the code (Part 2) to see how word embedding stacks up..."]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [conda root]", "language": "python", "name": "conda-root-py"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.3"}}, "nbformat": 4, "nbformat_minor": 5}