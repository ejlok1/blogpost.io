{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "### Code demo for Word Embeddings - Part 3\n",
    "Eu Jin Lok\n",
    "\n",
    "10 January 2018\n",
    "\n",
    "# How does word embeddings add value to predictive models\n",
    "In this notebook we will go into the details of how to create a custom sentiment lexicon step by step in code. For the full background on this topic, please checkout my blog post in this link: \n",
    "xxxxx\n",
    "\n",
    "This is part 3, where we create a visualisation of the word embeddings when trainned on the News Groups dataset. The dataset can be obtained from Kaggle:\n",
    "\n",
    "https://www.kaggle.com/crawford/20-newsgroups\n",
    "\n",
    "So without further ado, lets begin.... oh and Happy New year 2018! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [],
   "source": [
    "#import the key libraries \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from gensim.models import word2vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "import os \n",
    "import sys\n",
    "import sklearn.manifold #Tsne plot \n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\word embeddings\\\\\")\n",
    "np.random.seed(789)\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Just one function to create the plot \n",
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained word2vec object that we saved from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"W2V_model_save_300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now create the visualisation using T-SNE. The fitting takes awhile.... be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n",
    "all_word_vectors_matrix = model.wv.syn0\n",
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so its done. Lets annotate all the word with the fitted xy coordinates so we can plot on the map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[model.wv.vocab[word].index])\n",
    "            for word in model.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")\n",
    "    \n",
    "# Check a few points to see if it has done things correctly\n",
    "points.head(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets see the full scatter plot of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")    \n",
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So its all over the place. Key thing is that there should be clumps around which is a signal to us that those words are closely related in some way. So lets take a part of the map where we see clumps, and zoom in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the top right bit\n",
    "plot_region(x_bounds=(4, 5), y_bounds=(4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the top left part of the map shows us words that seem to point to sports concepts. Words like 'pitching', 'score', 'olympic', 'win', 'playoffs' just to name a few, are all words to do with sports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check the top left bit\n",
    "plot_region(x_bounds=(-5,-4), y_bounds=(-2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the top left part of the map shows us words that seem to point to computer concepts. Words like 'username', 'shell', 'etl', 'xdm', 'invoke' just to name a few, are all words to do with computers or IT. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
