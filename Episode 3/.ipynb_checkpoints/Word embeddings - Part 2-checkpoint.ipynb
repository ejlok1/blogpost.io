{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "## Code demo for Word Embeddings - Part 2\n",
    "Eu Jin Lok\n",
    "\n",
    "10 January 2018\n",
    "\n",
    "# How does word embeddings add value to predictive models\n",
    "In this notebook we will go into the details of how to build your own word embeddings, and use it as a powerful feature to improve you predictive model. For the full background on this topic, please checkout my blog post in this link: \n",
    "\n",
    "https://mungingdata.wordpress.com/2018/01/15/episode-3-word-embeddings/\n",
    "\n",
    "This is part 2, a continuing our journey from part 1 where we will now implement word embeddings and see how it can help improve our predictive model accuracy. The dataset we will use for this word embeddings is the news group dataset, can be found on Kaggle:\n",
    "\n",
    "https://www.kaggle.com/crawford/20-newsgroups\n",
    "\n",
    "Just to re-iterate, for the training dataset i'm using the News aggregator dataset. But I'm building the word embeddings on a totally different dataset that is news groups. I then extract features for the News aggregator dataset by referencing to the word embeddings build from news groups. Think of its as transfer learning. You could build word embeddings on the training dataset itself but i'm doing it this way to highlight the advantages of what word embeddings can offer. \n",
    "\n",
    "So without further ado, lets begin.... oh and Happy New year 2018! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import the key libraries \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler #normalise the word vectors \n",
    "from gensim.models import word2vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "import os \n",
    "import sys\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\word embeddings\\\\\")\n",
    "np.random.seed(789)\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok for this part, there's alot of functions that we need to create..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic text processing function for the news group dataset \n",
    "def normalize_text(s):\n",
    "    s = s.lower()\n",
    "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W\\s',' ',s)\n",
    "    # make sure we didn't introduce any double spaces\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    return s\n",
    "\n",
    "# Define a function to split a text into parsed sentences\n",
    "def text_to_sentences( texts, tokenizer, remove_stopwords=False ):\n",
    "    raw_sentences = tokenizer.tokenize(texts.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( text_to_wordlist( raw_sentence,remove_stopwords ))\n",
    "    return sentences\n",
    "\n",
    "# Word2Vec expects single sentences, each one as a list of words. In other words, the input format is a list of lists.\n",
    "def text_to_wordlist( texts, remove_stopwords=False ):\n",
    "    texts = re.sub(\"[^a-zA-Z]\",\" \", texts)             \n",
    "    texts = re.sub(r'\\b\\w{1,2}\\b', '', texts)  \n",
    "    words = texts.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "# wrapper for the news aggregator dataset (our original training dataset)\n",
    "def getCleanText(text):\n",
    "    clean_texts = []\n",
    "    for row in text['TITLE']:\n",
    "        clean_texts.append( text_to_wordlist( row, remove_stopwords=True ))\n",
    "    return clean_texts\n",
    "\n",
    "# Function to average the word vectors in a given text. \n",
    "# Ie. For each word in the text, Look-up the embeddings scores, then average them all thus obtaininig vectors at text level \n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])            \n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Calculate the average feature vector for each text, and loop across all text. \n",
    "def getAvgFeatureVecs(texts, model, num_features):\n",
    "    textFeatureVecs = np.zeros((len(texts),num_features),dtype=\"float32\")\n",
    "    counter = 0.\n",
    "    for text in texts:\n",
    "       if counter%100000. == 0.:\n",
    "           print (\"text %d of %d\" % (counter, len(texts)))\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(text, model, num_features)\n",
    "       counter = counter + 1.\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step we're going to build our word embeddings on a totally different dataset, and we're using news groups. We can of course build word embeddings on our original training dataset. But that's a copy out. Why not build it on a totally different dataset? That will show case the power of word embeddings more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "# read the news articles \n",
    "TEXT_DATA_DIR = 'C:\\\\Users\\\\User\\\\Documents\\\\GIT\\\\170411 Deep Learning NLP personal\\\\news20\\\\20_newsgroup\\\\'\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):  \n",
    "    path = os.path.join(TEXT_DATA_DIR, name)  \n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)      \n",
    "        labels_index[name] = label_id        \n",
    "        for fname in sorted(os.listdir(path)):  \n",
    "            if fname.isdigit():                 \n",
    "                fpath = os.path.join(path, fname)   \n",
    "                if sys.version_info < (3,):      \n",
    "                    f = open(fpath)             \n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')    \n",
    "                t = f.read()                              \n",
    "                i = t.find('\\n\\n')  # skip header          \n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)                       \n",
    "                f.close()                             \n",
    "                labels.append(label_id)              \n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word embeddings, we break them down to sentences for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "for row in texts:\n",
    "    sentences += text_to_sentences(row, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here comes the word2vec bit. Read more about it on my blog post. I'll just mentioned the key bits which is that i'm using a window context size of 10 (5 words before and 5 words after), 300 dimensions and using the CBOW method (as apposed to Skip-gram).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-14 15:28:59,912 : INFO : collecting all words and their counts\n",
      "2018-01-14 15:28:59,913 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-14 15:28:59,933 : INFO : PROGRESS: at sentence #10000, processed 120694 words, keeping 10390 word types\n",
      "2018-01-14 15:28:59,955 : INFO : PROGRESS: at sentence #20000, processed 244834 words, keeping 15153 word types\n",
      "2018-01-14 15:28:59,978 : INFO : PROGRESS: at sentence #30000, processed 375830 words, keeping 21862 word types\n",
      "2018-01-14 15:28:59,998 : INFO : PROGRESS: at sentence #40000, processed 492255 words, keeping 31045 word types\n",
      "2018-01-14 15:29:00,022 : INFO : PROGRESS: at sentence #50000, processed 594592 words, keeping 43912 word types\n",
      "2018-01-14 15:29:00,043 : INFO : PROGRESS: at sentence #60000, processed 715812 words, keeping 47341 word types\n",
      "2018-01-14 15:29:00,064 : INFO : PROGRESS: at sentence #70000, processed 833328 words, keeping 50530 word types\n",
      "2018-01-14 15:29:00,087 : INFO : PROGRESS: at sentence #80000, processed 972363 words, keeping 54548 word types\n",
      "2018-01-14 15:29:00,111 : INFO : PROGRESS: at sentence #90000, processed 1096124 words, keeping 60831 word types\n",
      "2018-01-14 15:29:00,134 : INFO : PROGRESS: at sentence #100000, processed 1220997 words, keeping 64705 word types\n",
      "2018-01-14 15:29:00,155 : INFO : PROGRESS: at sentence #110000, processed 1341637 words, keeping 67824 word types\n",
      "2018-01-14 15:29:00,177 : INFO : PROGRESS: at sentence #120000, processed 1462256 words, keeping 70592 word types\n",
      "2018-01-14 15:29:00,198 : INFO : PROGRESS: at sentence #130000, processed 1578160 words, keeping 72595 word types\n",
      "2018-01-14 15:29:00,221 : INFO : PROGRESS: at sentence #140000, processed 1700132 words, keeping 75406 word types\n",
      "2018-01-14 15:29:00,244 : INFO : PROGRESS: at sentence #150000, processed 1832234 words, keeping 77647 word types\n",
      "2018-01-14 15:29:00,269 : INFO : PROGRESS: at sentence #160000, processed 1972182 words, keeping 80072 word types\n",
      "2018-01-14 15:29:00,292 : INFO : PROGRESS: at sentence #170000, processed 2102049 words, keeping 82463 word types\n",
      "2018-01-14 15:29:00,314 : INFO : PROGRESS: at sentence #180000, processed 2227135 words, keeping 85212 word types\n",
      "2018-01-14 15:29:00,342 : INFO : PROGRESS: at sentence #190000, processed 2367149 words, keeping 89209 word types\n",
      "2018-01-14 15:29:00,366 : INFO : PROGRESS: at sentence #200000, processed 2510683 words, keeping 92245 word types\n",
      "2018-01-14 15:29:00,390 : INFO : PROGRESS: at sentence #210000, processed 2644499 words, keeping 94262 word types\n",
      "2018-01-14 15:29:00,413 : INFO : PROGRESS: at sentence #220000, processed 2775767 words, keeping 96277 word types\n",
      "2018-01-14 15:29:00,438 : INFO : PROGRESS: at sentence #230000, processed 2911789 words, keeping 98159 word types\n",
      "2018-01-14 15:29:00,462 : INFO : PROGRESS: at sentence #240000, processed 3043012 words, keeping 100558 word types\n",
      "2018-01-14 15:29:00,484 : INFO : PROGRESS: at sentence #250000, processed 3167017 words, keeping 102375 word types\n",
      "2018-01-14 15:29:00,508 : INFO : PROGRESS: at sentence #260000, processed 3305772 words, keeping 104744 word types\n",
      "2018-01-14 15:29:00,531 : INFO : PROGRESS: at sentence #270000, processed 3433812 words, keeping 106077 word types\n",
      "2018-01-14 15:29:00,554 : INFO : PROGRESS: at sentence #280000, processed 3571570 words, keeping 107512 word types\n",
      "2018-01-14 15:29:00,578 : INFO : PROGRESS: at sentence #290000, processed 3706534 words, keeping 108792 word types\n",
      "2018-01-14 15:29:00,602 : INFO : PROGRESS: at sentence #300000, processed 3836114 words, keeping 109878 word types\n",
      "2018-01-14 15:29:00,623 : INFO : PROGRESS: at sentence #310000, processed 3962917 words, keeping 110597 word types\n",
      "2018-01-14 15:29:00,639 : INFO : collected 111260 word types from a corpus of 4045326 raw words and 316542 sentences\n",
      "2018-01-14 15:29:00,640 : INFO : Loading a fresh vocabulary\n",
      "2018-01-14 15:29:00,694 : INFO : min_count=40 retains 8811 unique words (7% of original 111260, drops 102449)\n",
      "2018-01-14 15:29:00,695 : INFO : min_count=40 leaves 3580052 word corpus (88% of original 4045326, drops 465274)\n",
      "2018-01-14 15:29:00,716 : INFO : deleting the raw counts dictionary of 111260 items\n",
      "2018-01-14 15:29:00,720 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2018-01-14 15:29:00,721 : INFO : downsampling leaves estimated 2967778 word corpus (82.9% of prior 3580052)\n",
      "2018-01-14 15:29:00,722 : INFO : estimated required memory for 8811 words and 300 dimensions: 25551900 bytes\n",
      "2018-01-14 15:29:00,741 : INFO : resetting layer weights\n",
      "2018-01-14 15:29:00,842 : INFO : training model with 4 workers on 8811 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-01-14 15:29:01,856 : INFO : PROGRESS: at 9.95% examples, 1418671 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:02,858 : INFO : PROGRESS: at 19.39% examples, 1438364 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:03,865 : INFO : PROGRESS: at 29.60% examples, 1438469 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:04,866 : INFO : PROGRESS: at 38.80% examples, 1435801 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:05,870 : INFO : PROGRESS: at 48.93% examples, 1434137 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:06,871 : INFO : PROGRESS: at 58.27% examples, 1437315 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:07,875 : INFO : PROGRESS: at 69.04% examples, 1449392 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:08,881 : INFO : PROGRESS: at 78.22% examples, 1446098 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:09,884 : INFO : PROGRESS: at 88.48% examples, 1446791 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-14 15:29:10,886 : INFO : PROGRESS: at 97.65% examples, 1444051 words/s, in_qsize 6, out_qsize 1\n",
      "2018-01-14 15:29:11,117 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-14 15:29:11,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-14 15:29:11,126 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-14 15:29:11,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-14 15:29:11,129 : INFO : training on 20226630 raw words (14839842 effective words) took 10.3s, 1444615 effective words/s\n",
      "2018-01-14 15:29:11,130 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "#Initiate logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers\n",
    "                          , size=num_features\n",
    "                          , min_count = min_word_count\n",
    "                          , window = context\n",
    "                          , sample = downsampling\n",
    "                         )\n",
    "\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok so its done. Lets test a few words to see the results, just to make sure its working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('virgin', 0.8217429518699646),\n",
       " ('conception', 0.8204666972160339),\n",
       " ('immaculate', 0.794394850730896),\n",
       " ('prophet', 0.7263182401657104),\n",
       " ('angel', 0.7211571931838989),\n",
       " ('tomb', 0.7140588760375977),\n",
       " ('king', 0.7114769816398621),\n",
       " ('satan', 0.6937192678451538),\n",
       " ('luther', 0.6928719878196716),\n",
       " ('conceived', 0.6805691719055176)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"mary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('growth', 0.7403727173805237),\n",
       " ('increasing', 0.717843770980835),\n",
       " ('benefits', 0.7163976430892944),\n",
       " ('income', 0.6875376105308533),\n",
       " ('capacity', 0.6803438663482666),\n",
       " ('deficit', 0.6790786981582642),\n",
       " ('efficiency', 0.675233781337738),\n",
       " ('increase', 0.6728842854499817),\n",
       " ('losses', 0.6725849509239197),\n",
       " ('investment', 0.6717667579650879)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"economy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even better some visulisation... which is be provided in part 3 to reduce the lenght of this notebook: \n",
    "XXXXX\n",
    "\n",
    "I'm also going to save the model at this point for reuse. here's the code:\n",
    "model_name = \"W2V_model_save_300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "Now here's the magic moment where we get our original dataset (news aggregator), lookup the words to the word embeddings to get the context in 300 dimension, and use it as features by transforming it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 422419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 100000 of 422419\n",
      "Review 200000 of 422419\n",
      "Review 300000 of 422419\n",
      "Review 400000 of 422419\n",
      "(422419, 300)\n"
     ]
    }
   ],
   "source": [
    "# Read the original training dataset that is news aggregator\n",
    "news = pd.read_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\uci-news-aggregator.csv\")\n",
    "news['TEXT'] = [normalize_text(s) for s in news['TITLE']]\n",
    "\n",
    "# For each word in the news aggregator dataset (original training dataset), \n",
    "# get the average embedding score for each text, using the 300 dimensions   \n",
    "trainDataVecs = getAvgFeatureVecs( getCleanText(news), model, 300 )\n",
    "trainDataVecs[np.isnan(trainDataVecs)]=0\n",
    "\n",
    "# Normalized Data for use as features in  the predictive model\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(trainDataVecs)\n",
    "trainDataVecs = scaler.transform(trainDataVecs)\n",
    "print(trainDataVecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the word emebddings created. Lets find out how much improvement we'll get from it. So the first test, lets include both the usual word count vector features and the word embeddings and run through the same Multinomial Bayes as before in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(news['CATEGORY'])\n",
    "\n",
    "# Get the orignal features based on word count vectors \n",
    "vectorizer = CountVectorizer(max_features=300) \n",
    "x = vectorizer.fit_transform(news['TEXT']) \n",
    "\n",
    "# Combine both the word count vectors with the newly created word embeddings vectors  \n",
    "x = np.concatenate((trainDataVecs,x.toarray()), axis=1) # x can't be too large..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again lets take 0.1% of our dataset as training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61981009343668325"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train and test sets  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.999)\n",
    "\n",
    "# Multinomial NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is ~0.62, that's an improvement over ~0.58 that is just using word count vectors as features. Its not much you're right, but lets see what happens when we just use the word embeddings and leave out the word count vectors totally... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55678120934509012"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(trainDataVecs, y, test_size=0.999)\n",
    "\n",
    "# Multinomial NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is almost the same as using the word count vectors! Both are at 300 features so its a pretty equal comparison. \n",
    "\n",
    "Obviously, given the complexity and effort, one would wonder if its worthwhile at all. So let me reiterate again the a key point here. The word embeddings model is built on the NEWS GROUP dataset.... NOT the NEWS AGGREGATOR dataset. Typically one would built a word embeddings model directly on the same training dataset (news aggregator) but I wanted to go down a different path to show case how one could build a model off a totally different dataset, and then do a transfer learning quite simply. \n",
    "\n",
    "Read my blog to get a full debrief on this, a feel free to drop any comments or questions. :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
