{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42344751-df85-4fa9-af08-b4bf50b41e05"
    }
   },
   "source": [
    "# Episode 5: Building castles in the sky, or a memory palace Part 1 \n",
    "Eu Jin Lok\n",
    "\n",
    "17 March 2018\n",
    "\n",
    "# LSTM  \n",
    "In this notebook we will go into the details of how to build a document classifier using LSTM, a deep learning architecture that is able to remember long-term dependencies. For the full background on this topic, please checkout my blog post in this link: \n",
    "\n",
    "https://mungingdata.wordpress.com/2018/03/21/episode-5-building-castles-in-the-sky-or-a-memory-palace-part-1/\n",
    "\n",
    "This dataset is based on Episode 4 and builds upon the previous CNN architecture. So without further ado, lets begin...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "6b931974-2fc8-4fdc-9f32-a93d21ec08ef"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the key libraries \n",
    "import pandas as pd \n",
    "from pandas import crosstab\n",
    "import numpy as np\n",
    "import os \n",
    "import pickle\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Flatten, MaxPooling1D, SpatialDropout1D, Dropout,Convolution1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "os.chdir(\"C:\\\\Users\\\\User\\\\Dropbox\\\\Pet Project\\\\Blog\\\\DONE CNN\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first step after loadings the necessary packages, we'll go grab our training dataset, the same one from the previous Episode 4, and I just copy the code here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23313 unique tokens or words.\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "train = pd.read_csv(\"happydb\\\\cleaned_hm.csv\")  \n",
    "\n",
    "# Lets one-hot encode the labels  \n",
    "labels=train.predicted_category.unique()\n",
    "dic={}\n",
    "for i,labels in enumerate(labels):\n",
    "    dic[labels]=i\n",
    "labels=train.predicted_category.apply(lambda x:dic[x])\n",
    "\n",
    "val=train.sample(frac=0.2,random_state=200)\n",
    "train=train.drop(val.index)\n",
    "\n",
    "NUM_WORDS=20000 # if set, tokenization will be restricted to the top num_words most common words in the dataset).\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "\n",
    "# we need to fit the tokenizer on our text data in order to get the tokens\n",
    "texts=train.cleaned_hm\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Found %s unique tokens or words.' % len(word_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the words the sentences in our documents to the index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a successful date with someone I felt sympathy and connection with.\n",
      "[1, 23, 16, 3, 758, 315, 13, 284, 1, 94, 9298, 5, 2393, 13]\n",
      "315 = index for the word 'Date' \n",
      "Date is an index number of 315. And it appears in the right position (5th) in the sentence\n"
     ]
    }
   ],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(texts) # converts the text to numbers essentially\n",
    "sequences_valid=tokenizer.texts_to_sequences(val.cleaned_hm)\n",
    "word_index = tokenizer.word_index\n",
    "#Although word_index contains all words tokenizer.texts_to_sequences takes num_words into account.\n",
    "\n",
    "# Check the index is working correctly \n",
    "print(texts[0])\n",
    "print(sequences_train[0])\n",
    "print(word_index['date'],\"= index for the word 'Date' \") \n",
    "print('Date is an index number of 315. And it appears in the right position (5th) in the sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting the data into an LSTM architecture, and we need to ensure the shape of the dataset is the same across all text. But because each text varies in lenght, we'll cap it at a fixed length, and just pad it with zeros to fill in the gaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1   23   16    3\n",
      "  758  315   13  284    1   94 9298    5 2393   13]\n"
     ]
    }
   ],
   "source": [
    "# set the sequence length of the text to speed up training and prevent overfitting. \n",
    "seq_len = 500\n",
    "X_train = pad_sequences(sequences_train,maxlen=seq_len, value=0)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=seq_len, value=0)\n",
    "\n",
    "# Lets check a single record to see how it looks\n",
    "print(X_train[0]) # By default we pad the left side. In order words, all the text is right side aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last part of the processing is to one-hot encode / binarise the target. That's the format that works well with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80428, 7)\n",
      "(20107, 7)\n"
     ]
    }
   ],
   "source": [
    "y_train =train.predicted_category.apply(lambda x:dic[x])\n",
    "y_train = to_categorical(np.asarray(labels[train.index]))\n",
    "y_val =val.predicted_category.apply(lambda x:dic[x])\n",
    "y_val = to_categorical(np.asarray(labels[y_val.index]))\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM layer only = 97% accuracy\n",
    "And here we are. Since this notebook is going to be short, I thought I'll showcase a few variations of LSTM. Lets start with a simple LSTM without using a pretrained embedding. \n",
    "\n",
    "WARNING: I'm using my desktop computer which is calibrated for CUDA processing. The timing printed below, will vary depending on your hardware specification. I've printed my GPU specs below. A CPU will take 10 times longers generally... and LSTM takes a long long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104815001\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16597597132222788778\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = list(device_lib.list_local_devices())\n",
    "print(device[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 4,177,599\n",
      "Trainable params: 4,177,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# Use a simple LSTM structure\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(7, activation='sigmoid'))  # 7 targets, each done as a logistic  \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup a checkpoint to ensure we save the best solution, and an early stopping procedure. And run the model for just 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 752s 12ms/step - loss: 0.1706 - acc: 0.9371 - val_loss: 0.1077 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96011, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 747s 12ms/step - loss: 0.1001 - acc: 0.9650 - val_loss: 0.0948 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96011 to 0.96440, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 749s 12ms/step - loss: 0.0764 - acc: 0.9725 - val_loss: 0.0800 - val_acc: 0.9702\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96440 to 0.97025, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 745s 12ms/step - loss: 0.0589 - acc: 0.9790 - val_loss: 0.0826 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97025 to 0.97075, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 738s 11ms/step - loss: 0.0500 - acc: 0.9827 - val_loss: 0.0784 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97075 to 0.97311, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19efd09a9e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97%, and seems like it could continue to improve further... but lets stop at 5 epochs and you can try it yourself. Note that LSTM takes incrementally longer when compared to CNN. Now lets confirm the accuracy by applying it to the actual validation dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 119s 6ms/step\n",
      "0.9743870398629136\n"
     ]
    }
   ],
   "source": [
    "# Load the model from epoch 1, which is the best. If we use the latest model from Epoch 5, accuracy is terrible. Guess why?\n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.4% accuracy! This is our best model so far! Now lets see how we what happens when we use a hybrid model of CNN and LSTM..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN and LSTM layers = 97%\n",
    "Start with the standard embedding layer first, then followed by a Convolution layer followed by an LSTM. A hybrid model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 2,440,271\n",
      "Trainable params: 2,440,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# Use a Convolution Kernal first then LSTM \n",
    "model.add(e)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup checkpoint..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 344s 5ms/step - loss: 0.1974 - acc: 0.9248 - val_loss: 0.1238 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94803, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_LSTM.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 342s 5ms/step - loss: 0.1054 - acc: 0.9615 - val_loss: 0.0984 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94803 to 0.96330, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_LSTM.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 345s 5ms/step - loss: 0.0776 - acc: 0.9731 - val_loss: 0.0944 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96330 to 0.96621, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_LSTM.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 343s 5ms/step - loss: 0.0614 - acc: 0.9787 - val_loss: 0.0935 - val_acc: 0.9683\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96621 to 0.96835, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_LSTM.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 341s 5ms/step - loss: 0.0508 - acc: 0.9823 - val_loss: 0.0849 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96835 to 0.97027, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.CNN_LSTM.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19efe143128>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.CNN_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 52s 3ms/step\n",
      "0.9728026601079742\n"
     ]
    }
   ],
   "source": [
    "# Load the model from epoch 1, which is the best. \n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.2% accuracy! So slightly lower. But generally about the same as just using one LSTM layer... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double LSTM layers = 97%\n",
    "Now lets try a double LSTM layer .... this might take awhile to finish..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 100)          2331400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 500, 100)          80400     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 2,492,907\n",
      "Trainable params: 2,492,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential setup \n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, input_length=seq_len)\n",
    "\n",
    "# After the embedding layer, use an LSTM and then another LSTM. First LSTM returns the sequence length as outputs \n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences = True))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary()) # summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64342 samples, validate on 16086 samples\n",
      "Epoch 1/5\n",
      "64342/64342 [==============================] - 1314s 20ms/step - loss: 0.2367 - acc: 0.9089 - val_loss: 0.1357 - val_acc: 0.9450\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94505, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM_LSTM.hdf5\n",
      "Epoch 2/5\n",
      "64342/64342 [==============================] - 1277s 20ms/step - loss: 0.1272 - acc: 0.9528 - val_loss: 0.1209 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94505 to 0.95187, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM_LSTM.hdf5\n",
      "Epoch 3/5\n",
      "64342/64342 [==============================] - 1272s 20ms/step - loss: 0.1019 - acc: 0.9635 - val_loss: 0.0997 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95187 to 0.96346, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM_LSTM.hdf5\n",
      "Epoch 4/5\n",
      "64342/64342 [==============================] - 1270s 20ms/step - loss: 0.0849 - acc: 0.9711 - val_loss: 0.0986 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.96346 to 0.96561, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM_LSTM.hdf5\n",
      "Epoch 5/5\n",
      "64342/64342 [==============================] - 1283s 20ms/step - loss: 0.0727 - acc: 0.9753 - val_loss: 0.0854 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96561 to 0.96843, saving model to C:\\Users\\User\\Downloads\\dump\\weights_base.LSTM_LSTM.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6054043c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup checkpoint \n",
    "file_path=\"C:\\\\Users\\\\User\\\\Downloads\\\\dump\\\\weights_base.LSTM_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20107/20107 [==============================] - 195s 10ms/step\n",
      "0.970770674851732\n"
     ]
    }
   ],
   "source": [
    "# Load the model from epoch 1, which is the best. \n",
    "model.load_weights(file_path) \n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=1) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% accuracy! Not as good as the previous model, but only marginally different. End of the day... all equally good"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
